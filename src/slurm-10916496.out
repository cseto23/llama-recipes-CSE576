
=====================================================================
This module is intended solely for building or source activating user
python environments, i.e.,

    mamba create -n myenv -c conda-forge

or

    source activate myenv

To list available environments, run:

    mamba info --envs

See our docs: https://links.asu.edu/solpy

Any other use is NOT TESTED.
=====================================================================

  
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:10<01:01, 10.29s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:19<00:46,  9.38s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:27<00:36,  9.01s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:36<00:26,  9.00s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:50<00:21, 10.75s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:59<00:10, 10.27s/it]Loading checkpoint shards: 100%|██████████| 7/7 [01:08<00:00,  9.68s/it]Loading checkpoint shards: 100%|██████████| 7/7 [01:08<00:00,  9.75s/it]
slurmstepd: error: *** JOB 10916496 ON g042 CANCELLED AT 2023-12-01T18:41:07 DUE TO TIME LIMIT ***
